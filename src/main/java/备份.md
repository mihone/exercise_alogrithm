商品检索系统
# 背景
1. 为什么要做？
随着商品，商家不断增长，以及供给结构的变化，
连锁商家的商品管理愈加重要。
2. 原来有没有，有什么问题？
业务未来期望在质量，分析导购等多个方面帮助商家商品赋能，因此需要一个支持多维度复杂条件检索的能力。

这类检索诉求通常不是对商品上单个字段的值检索，它的形成非常非常复杂，首先数据来源杂，可能是其他系统，或者其他部门，或者数仓，
其次业务需求的检索条件背后有一系列逻辑规则，并且未来可能会经常变更。


[//]: # (随着业务体量增大，发展迅速，未来业务期望在质量，分析，导购等多个方面为几十个品类的商家赋能，为此，需要一个可以支持多维度复杂条件灵活检索的基建类)

[//]: # (检索能力，并可以快速更新迭代。然而当前的商品检索以单个店铺为维度，虽然有降级方案实现跨店铺查询，但性能差，条件少，加条件成本高，无法满足业务发展诉求。)

# 挑战
简单来说就是业务复杂，需要优化流程，降低开发成本，提高效率来支持业务发展。
1. 在新检索条件开发上，如何降本增效，避免烟囱式开发？
2. 保证系统稳定和数据准确。（其他挑战）

[//]: # (1. 如何设计架构和方案，可以让支持高性能的复杂条件的商家维度和店铺维度维度检索，且后续检索条件开发成本低，更新维护便捷，避免烟囱式开发？)


# 做法
在做法上，业界支持复杂条件检索的中间件有ES，和分析型数据库，比如clickhouse，doris，从即时性，使用场景考虑，选用了ES作为检索中间件。
然后就产生了一个问题，通过什么形式和流程把数据写入es，一种方案其实是监听商品变更直接写入，这样设计虽然简单，但由于商品变更消息量大，且峰值高，高峰值对es
性能会有一定影响，另外后面如果出现问题，很难排查。需要有个其他存储介质来协助，这里也是参考了cqrs架构，先根据商品变更异构一份商品信息，再监听异构的数据的binlong
写入es，好处有三点，1，异构过程中可以合并一些同一商品变更消息，降低流量，2，方便后面做数据校验，问题排查。
在如何设计检索条件的存储结构上，综合考虑维护便利和有es查询语句复杂嵌套带来的查询性能因素，摈弃了原来索引的横向设计，
将检索条件转化为标签，通过标签的交并补逻辑，以及标签的层级关系，完成对商品的检索。
这样问题转变成了如何设计标签的生产流程，可以支持快速迭代，开发维护成本低。
解决这个的思路有两个方向，配置化和自动化。
第一个 ，考虑标签本身是个比较通用的概念 很自然想到配置化，，而从配置的人角度考虑，可以使开发，可以使产品运营，从这两类人群考虑，设计了两个配置模式，一种是页面提供多种选项，满足条件即可打标，实现形式是根据条件进行数据圈选，
主要面向产品运营。
另一种面向开发，通过aviator表达式，支持复杂的逻辑条件，满足需要上下文信息来进行复杂判断的场景。
第二个，自动化，这涉及了整个系统数据的流转形式和流程。整个商品检索系统有三个模块，圈选模块，标签生产模块，数据读写模块。
标签生产模块会监听其他系统的kafka消息，基于上面说的配置化的标签规则，生产标签。生产的标签存入db，并将产生binlog以消息的形式对外通知。
数据读写模块则监听商品信息和标签信息的变更，写入es。

这里面有两个点，配置变更和外部数据变更如何实现的自动化。
首先，我会监听配置变更的binlog，配置变更会触发数据重新圈选，然后圈选的结果消息形式通知标签生产模块，标签生产模块组装上下文重新计算标签。
然后输出最终的商品标签结果，商品标签结果再通知数据读写模块进行es的更新。
第二个点，外部数据变更，比如说hive 的更新，其他系统产出的数据更新，我都统一使用消息来接收，这样外部的更新我们无须关注，只要外部系统发出变更通知
我会自动重算标签，更新es。

# 成果  
首先应用于连锁商家的商品管理，并完成与质量系统对接打通， 在质量域上为商家的商品质量提高赋能。
后面预期商家的商品分析等方向。

索引里数据 的上游，全部为kafka，换句话说，索引里会有商品信息，标签信息，都是通过kafka流转过来的。
而

------
## 项目简单介绍
背景 
随着连锁商家的增长，业务上期望通过多个方向不断为商家赋能，为此需要一个系统，支持多维度复杂条件检索能力。
这个项目最大的挑战是如何设计架构和方案，可以满足为商家提供多维度的复杂查询，并且支持业务快速接入和迭代，避免烟囱式开发
，长期导致开发成本越来越高。
做法是，建立一个新的检索系统，以es作为提供检索的终端存储，并抛弃以前横向设计，将检索条件标签化，并通过脚本配置或运营规则配置两种方式
来将标签配置化，并通过监听变更来完成检索条件--标签的自动更新。
成果是，为多品类商家提供了丰富的检索能力，并与质量等系统对接为商家管理商品赋能。
------


这里也考虑到了es写入性能，

难点
1、重建一套存储结构，如何保证重建过程中的稳定性和数据的一致性？
前面说定了终端存储是ES，所以问题就是在有大流量商品增删改的过程中，怎么保证准确和一致。
一般常见的导数的方式是写个脚本来写。但写的时候无法避免增量数据的更新
而且es对并发更新的支持并不是非常好，容易导致数据不一致。
所以就要尽量减少这种并发更新，产生了存量写入和增量变更消息在写es之前能合并转化成一个变更消息，这样可以避免es上的并发更新。
一种方案是，脚本不直接写es，模拟出binlog的消息，但成本较高，另外后续增量对比校准比较麻烦。
这里借鉴了借鉴了cqrs架构的思路，选用db承接，同时考虑到商品信息未来可能用来其他查询，所以es前面选择用异构一份db数据来，es监听这个db的binlog完成写入。
。
在db选型上，之前db采用分库分表设计，一方面是直接面向用户，需要更快响应，且保证数据强一致性。
我这边由于db不直接对客户提供查询，未来的使用场景大概率是在打标过程中反查一些信息用于标签计算，对绝对的查询耗时不是特别敏感，且考虑到异构db的维护成本
选用了分布式事务性数据库，市面对标产品类比的话，tidb，OceanBase。
写入策略上，采用insert on duplicate key update。解决存量写入和增量更新时的冲突问题。
当然从binlog到es的写入，然后把要写入es的数据体进行组装，会按照商品id进行分片和转发，保证同一个文档id的并发写最低。

上线初期每天会有定时任务比对异构db，es的数据，保证准确。

2、怎么解决高级标签打标不准，慢的问题。


# 自问自答
1	供给结构变化是指？
基于Lbs的商家有多个店铺，每个店铺有独立价格库存，随着大品牌的扩张，商家商品中连锁占比非常高，占了大半。
2、发布一个标签，打了一部分有问题怎么办
首先，新标签上线会有灰度，即把灰度的部分店铺作为标签啊规则的一部分，如果发现有问题，可以修改规则触发重新打标，来消掉。
并且考虑到同时有其他标签的打标消标也在同时进行，我们在写入es之前有标签优先级机制，通过配置标签变更消费的优先级，来让这部分标签快速消掉。
尽可能降低风险影响时间。
3、优化点
业务角度思考，考虑标签风险需要审核。
技术角度，削减标签执行粒度，判断哪些需要执行，需要哪些上下文信息，提高标签执行效率



成就
完成了质量信息和检索系统的打通

对于这个系统，还有什么可以优化的点：
1、打标是打全部标签


-------
复盘问题
#### 1、商品模型是什么样的，统一模型有什么好处和劣势。
优势：
 1.1 提高迭代效率，减少重复开发，统一数据结构可以降低开发复杂性，核心功能改进后续可以直接复用到所有业务线
加速功能上线。
 1.2 维护成本降低
     只有一套模型规范，降低模型冲突等潜在风险，同时减少沟通和培训成本，
 1.3 长期隐形优势
    后续业务线扩展，边际成本逐渐降低，形成平台化能力，避免未来重构风险。
劣势：
  设计复杂度高，统一模型未必可以一次性满足所有需求，设计通用性强但不失扩展性会导致初期开发周期延长。
随着业务差异的迭代，一些冗余字段可能会污染结构。
针对劣势的解决办法：
  模型分为核心模型和扩展模型。
  核心模型为全业务通用结构化形式，扩展模型则可以考虑结构+非结构化的形式，来满足需求。 
  并通过trace等方式为业务定制身份，做业务隔离。
#### 2、旅游商品模型有什么特点
旅游商品标准化程度低，一个旅游商品会有多个不同领域的商品组合而成，（这不同领域的商品一般都称作是资源）
每个领域的资源有不同定义和设计。
举个例子，电商商品相当于一个车，车有门，有玻璃，有导航等等。
而旅游商品相当于一个包，包里有笔，有书，有手机，

2、你了解哪些c端的设计和架构？
  1、多级缓存
      本地缓存，redis 减少数据库压力，通过异步，比如binlog进行实时更新
  2、数据库上
     cqrs架构，读写分离
      数据库按照业务模块，用户信息等分库分表，避免单一库表压力过大
  3、考虑异步场景，对大流量削峰填谷
  4、有降级策略，返回兜底数据或者提示。
  5、冷热数据分离，最热数据缓存，其次db再其次hbase等归档。 
3、项目难点不够。
    重要！！！！
    这分为技术难点和业务难点
    业务难点就是刚才说的，业务复杂， 检索条件产生来源，规则复杂且迭代快，接入方多，需要降低开发成本。
    技术上的难点，我说一个在后续迭代过程中遇到的一个。
    前面说商家商品中连锁占比高，连锁商家特点是有总部商品和门店商品，总部商品和门店商品通过一定的关系进行关联。
    然后有一个场景是要实时检索总部商品 关联的门店商品数量，并且要区分出这里面哪些有【优质商品】标签，哪些没有
    一种方案是直接去es查，但是这个查询动作跨多个分片，高峰时期响应耗时慢，严重影响用户体验。
    另一种方案是用tair记录这个总部商品下门店商品有优质标签的门店，这个可以解决查询慢的问题
    但也会有一个问题，tair的数据膨胀非常大，变成了商品*门店的量级，成本太高
    想到这个是否有优质商品标签本身类似01问题，因此采用了redis的bitmap，如果每个总部商品一个bitmap，门店商品优质则1，否则为0
    缓存里维护一份门店和bit位的map，当门店商品打标或者消标的时候，更新bitmap里的bit位状态。来实现快速的统计。
    同时，为了保证数据一致性
    考虑到门店可能会增加和减少，监听这部分的变更，新增门店则在原来基础上追加，删除门店则清空对应bit位的数据。

    

4、介绍一些tips规避是自己的问题导致。比如说为什么要重建索引，不要说之前设计的问题。
    标签数据量增长快+考虑单一职责问题，服务模块化，把一些标签信息单独拆出来。
    并且要为后续集群稳定性考虑，更换集群





-----

06.27反问
+1
1、从jd上了解到是做内容平台的，想了解下，这个业务的流转的生命周期是什么样的？
数据哪里来，怎么处理，目标客户等。
2、jd上说是要结合ai来赋能业务，是什么样的形式
3、是能接触到大模型应用上的工程和技术嘛
4、如果进去了进入之后还会有大佬牵头带着搞么？
5、日常上下班时间
内容平台怎么衡量产出。
组内成员
-------
7.1 反问
主要是做



